{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf0edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import shapely\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.geodesic as cgeo\n",
    "import cartopy.vector_transform as cvt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from shapely import prepare, Point, Polygon\n",
    "from shapely.validation import make_valid\n",
    "from skimage import measure, morphology\n",
    "import sqlite3\n",
    "from oceannavigator.dataset_config import DatasetConfig\n",
    "from data import open_dataset\n",
    "from data.sqlite_database import SQLiteDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_keys= DatasetConfig.get_datasets()\n",
    "for dataset in dataset_keys:\n",
    "    try:\n",
    "        config= DatasetConfig(dataset)\n",
    "        url= config.url\n",
    "        with SQLiteDatabase(url) as db:\n",
    "            variable_list= db.get_data_variables()\n",
    "            variable=variable_list[0].key\n",
    "            timestamp=db.get_latest_timestamp(variable)\n",
    "            files = db.get_netcdf_files([timestamp], [variable])\n",
    "            src_path = files[-1]\n",
    "            filename = os.path.basename(src_path)\n",
    "            if not os.path.exists(filename):\n",
    "                shutil.copy(src_path, filename)\n",
    "            ds= xr.open_dataset(filename)\n",
    "            var=ds[variable]\n",
    "        # get surface level data\n",
    "        if len(ds.dims)==3:\n",
    "            surface_data = var[0, :, :].data\n",
    "        elif len(ds.dims)==2:\n",
    "            surface_data= var.data\n",
    "        else:\n",
    "            surface_data = var[0, 0, :, :].data\n",
    "\n",
    "        # create binary mask from data\n",
    "        binary_mask = np.where(np.isnan(surface_data), 0, 1)\n",
    "        binary_mask = np.pad(binary_mask, 1)  # pad the mask so that the edges will be included in the perimeter\n",
    "\n",
    "        # create a convex hull mask\n",
    "        ch_mask = morphology.convex_hull_image(binary_mask)\n",
    "\n",
    "        # get the contours from the mask\n",
    "        contours = measure.find_contours(ch_mask, level=0)\n",
    "\n",
    "        # select the first contour for our perimeter (the first element should be the one we're interested in but you'll have to confirm yourself)\n",
    "        perim_y, perim_x = np.transpose(contours[0]).astype(int)\n",
    "\n",
    "        # shift coordinates on array edges so that we're not selecting the padded portion\n",
    "        height, width = ch_mask.shape\n",
    "\n",
    "        perim_y[perim_y == 0] = 1\n",
    "        perim_y[perim_y >= height - 1] = height - 2\n",
    "\n",
    "        perim_x[perim_x == 0] = 1\n",
    "        perim_x[perim_x >= width - 1] = width - 2\n",
    "\n",
    "        #second version\n",
    "        lat_var = ds.get(\"latitude\", ds.get(\"lat\"))\n",
    "        lon_var=ds.get(\"longitude\", ds.get(\"lon\"))\n",
    "        dim = lat_var.ndim\n",
    "        # Select that actual lon lat values\n",
    "        if dim==2:\n",
    "            pad_lat = np.pad(lat_var.data, 1)\n",
    "            pad_lon = np.pad(lon_var.data, 1)\n",
    "            lon_mesh = lon_var.data\n",
    "            lat_mesh = lat_var.data\n",
    "\n",
    "        elif dim==1:\n",
    "            lon_mesh, lat_mesh = np.meshgrid(ds.longitude.data, ds.latitude.data)\n",
    "            pad_lat=np.pad(lat_mesh,1)\n",
    "            pad_lon=np.pad(lon_mesh,1)\n",
    "\n",
    "\n",
    "        pts = np.stack([lon_mesh, lat_mesh], axis=2)\n",
    "        pts = np.apply_along_axis(lambda pt: Point(pt), 2, pts)\n",
    "        perim_lat = pad_lat[perim_y, perim_x]\n",
    "        perim_lon = pad_lon[perim_y, perim_x]\n",
    "        perim_poly = Polygon(np.stack([perim_lon, perim_lat], axis=1))\n",
    "        prepare(perim_poly)\n",
    "        poly_mask = perim_poly.contains_properly(pts).astype(bool)\n",
    "\n",
    "        #third version\n",
    "\n",
    "        idx = np.argmax(np.abs(np.diff(perim_lon)))\n",
    "        if shapely.is_simple(perim_poly)==False:\n",
    "            new_lons = [360, 360, 0, 0]\n",
    "            new_lats = [perim_lat[idx], 90, 90, perim_lat[idx + 1]]\n",
    "            perim_lon = np.insert(perim_lon, idx + 1, new_lons)\n",
    "            perim_lat = np.insert(perim_lat, idx + 1, new_lats)\n",
    "\n",
    "        name = config.name.strip().replace(\" \", \"_\") + \".pkl\"\n",
    "        with open(name, \"wb\") as f:\n",
    "            pickle.dump(perim_poly, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} for dataset: {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e40b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.name, \"wb\") as f:\n",
    "    pickle.dump(perim_poly, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_keys= DatasetConfig.get_datasets()\n",
    "config= DatasetConfig(dataset_keys[2])\n",
    "with open(config.name, \"rb\") as f:\n",
    "    poly = pickle.load(f)\n",
    "\n",
    "point = Point([-65, 79])\n",
    "\n",
    "print(poly.contains(point))\n",
    "print(poly.boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97471e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_keys= DatasetConfig.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=dataset_keys[2]\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from oceannavigator import DatasetConfig\n",
    "from data.sqlite_database import SQLiteDatabase\n",
    "from dateutil.parser import parse as dateparse\n",
    "from data.utils import get_data_vars_from_equation, time_index_to_datetime\n",
    "import numpy as  np\n",
    "\n",
    "\n",
    "for key in DatasetConfig.get_datasets():\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        config = DatasetConfig(key)\n",
    "        target_date='2025-07-23T02:30:00.000Z'\n",
    "        parsed_date = dateparse(target_date)\n",
    "        with SQLiteDatabase(config.url) as db:\n",
    "            sample_var = config.variables[0]\n",
    "            vals = np.asarray(db.get_timestamps(sample_var))\n",
    "            if vals.size==0:\n",
    "                continue\n",
    "\n",
    "            time_dim_units = config.time_dim_units\n",
    "            converted_times = time_index_to_datetime(vals[[0, -1]], time_dim_units)\n",
    "\n",
    "            if (\n",
    "                converted_times[0] <= parsed_date\n",
    "                and converted_times[1] >= parsed_date\n",
    "            ):\n",
    "                print(f\"Data Matched for {key}\")\n",
    "        t1 = time.time()\n",
    "        print(f\"time taken for {key} is {t1 - t0}\")\n",
    "    except Exception as e:\n",
    "         continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from oceannavigator import DatasetConfig\n",
    "from data.sqlite_database import SQLiteDatabase\n",
    "from dateutil.parser import parse as dateparse\n",
    "from data.utils import get_data_vars_from_equation, time_index_to_datetime\n",
    "import numpy as  np\n",
    "\n",
    "\n",
    "for key in DatasetConfig.get_datasets():\n",
    "    try:\n",
    "  \n",
    "        config = DatasetConfig(key)\n",
    "        target_date='2025-07-23T02:30:00.000Z'\n",
    "        parsed_date = dateparse(target_date)\n",
    "        with SQLiteDatabase(config.url) as db:\n",
    "            sample_var = config.variables[0]\n",
    "            t0 = time.time()\n",
    "            vals = np.asarray(db.get_timestamps(sample_var))\n",
    "            t1 = time.time()\n",
    "            if vals.size==0:\n",
    "                continue\n",
    "\n",
    "            time_dim_units = config.time_dim_units\n",
    "            converted_times = time_index_to_datetime(vals[[0, -1]], time_dim_units)\n",
    "            print(\"converted:\",converted_times)\n",
    "            print(\"parsed\",parsed_date)\n",
    "      \n",
    "            \n",
    "\n",
    "            if (\n",
    "                converted_times[0] <= parsed_date\n",
    "                and converted_times[1] >= parsed_date\n",
    "            ):\n",
    "                print(f\"Data Matched for {key}\")\n",
    "     \n",
    "    except Exception as e:\n",
    "         continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cf87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=DatasetConfig.get_datasets()\n",
    "config = DatasetConfig(key[2])\n",
    "print(config.variable[\"votemper\"].scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dafc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ad35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with SQLiteDatabase(config.url) as db:\n",
    "    t0 = time.time()\n",
    "    db.get_earliest_timestamp(config.variables[0])\n",
    "    t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "for key in DatasetConfig.get_datasets():\n",
    "    config = DatasetConfig(key)\n",
    "    try:\n",
    "        if not isinstance(config.url,list) and config.url.endswith(\".sqlite3\"):\n",
    "            pass\n",
    "        else:\n",
    "            if not isinstance(config.url, list):\n",
    "                data=xr.open_mfdataset([config.url])\n",
    "            else:\n",
    "                data=xr.open_mfdataset(config.url)\n",
    "            time_vals=data.time.values\n",
    "            first_time = pd.to_datetime(time_vals[0]).tz_localize('UTC')\n",
    "            last_time = pd.to_datetime(time_vals[-1]).tz_localize('UTC')\n",
    "            if first_time <= parsed_date and last_time >= parsed_date:\n",
    "                matching_dataset_ids.append(dataset_id)\n",
    "\n",
    "            print(data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{e} dataset {key} config url {config.url}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "ds=xr.open_mfdataset([\"https://salishsea.eos.ubc.ca/erddap/griddap/ubcSSg3DBiologyFields1hV21-11\"])\n",
    "time_vals=ds.time.values\n",
    "first_timestamp = time_vals[0].astype('datetime64[s]').astype(int)\n",
    "last_timestamp = time_vals[-1].astype('datetime64[s]').astype(int)\n",
    "\n",
    "first_time = datetime.fromtimestamp(first_timestamp, tz=timezone.utc)\n",
    "last_time = datetime.fromtimestamp(last_timestamp, tz=timezone.utc)\n",
    "target_date='2025-07-23T02:30:00.000Z'\n",
    "parsed_date = dateparse(target_date)\n",
    "if first_time <= parsed_date and last_time >= parsed_date:\n",
    "        print(first_time)\n",
    "        print(parsed_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe4b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m variable_list = []\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset_keys = \u001b[43mDatasetConfig\u001b[49m.get_datasets()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset_key \u001b[38;5;129;01min\u001b[39;00m dataset_keys:\n\u001b[32m      5\u001b[39m     config = DatasetConfig(dataset_key)\n",
      "\u001b[31mNameError\u001b[39m: name 'DatasetConfig' is not defined"
     ]
    }
   ],
   "source": [
    "variable_list = []\n",
    "dataset_keys = DatasetConfig.get_datasets()\n",
    "\n",
    "for dataset_key in dataset_keys:\n",
    "    config = DatasetConfig(dataset_key)\n",
    "    for variable in config.variables:\n",
    "        variable_list.append(config.variable[variable].name)\n",
    "\n",
    "variable_list = list(dict.fromkeys(variable_list))\n",
    "print(variable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_keys = DatasetConfig.get_datasets()\n",
    "config = DatasetConfig(dataset_keys[2])\n",
    "for var_key in config.vector_variables.keys():\n",
    "    var_data = config.vector_variables[var_key]\n",
    "    var_name = var_data.get(\"name\", var_key)\n",
    "var_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_variable_list=[]\n",
    "for var_key in config.vector_variables.keys():\n",
    "    var_data = config.vector_variables[var_key]\n",
    "    var_name = var_data.get(\"name\", var_key)\n",
    "    vector_variable_list.append(var_name)\n",
    "vector_variable_list=list(dict.fromkeys(vector_variable_list))\n",
    "print(vector_variable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d34ebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for axis 0 with size 0 for giops_fc_10d_2dll\n",
      "index 0 is out of bounds for axis 0 with size 0 for riops_fc_3dps\n",
      "matchedciops-east_fc_3dll\n",
      "matchedciops-east_fc_2dll\n",
      "matchedciops-west_fc_3dll\n",
      "matchedciops-west_fc_2dll\n",
      "matchedciops-salish-sea_fc_3dll\n",
      "matchedciops-salish-sea_fc_2dll\n",
      "unable to open database file for cmems_climatology\n",
      "unable to open database file for cmems_monthly_climatology\n",
      "unable to open database file for cmems_seasonal_climatology\n",
      "matchedsalishseacast_3d_biology\n",
      "matchedsalishseacast_3d_currents\n"
     ]
    }
   ],
   "source": [
    "dataset_id_list=DatasetConfig.get_datasets()\n",
    "target_date='2025-07-23T02:30:00.000Z'\n",
    "parsed_date = dateparse(target_date)\n",
    "\n",
    "for dataset_id in dataset_id_list:\n",
    "    try:\n",
    "        config = DatasetConfig(dataset_id)\n",
    "        if not isinstance(config.url,list) and config.url.endswith(\".sqlite3\"):\n",
    "            with SQLiteDatabase(config.url) as db:\n",
    "                sample_var = config.variables[0]\n",
    "                vals = np.array(db.get_timestamps(sample_var))\n",
    "\n",
    "                time_dim_units = config.time_dim_units\n",
    "                converted_times = time_index_to_datetime(\n",
    "                    vals[[0, -1]], time_dim_units\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                    converted_times[0] <= parsed_date\n",
    "                    and converted_times[1] >= parsed_date\n",
    "                ):\n",
    "                    print(f\"matched{dataset_id}\")\n",
    "        else:\n",
    "            if not isinstance(config.url, list):\n",
    "                data=xr.open_mfdataset([config.url])\n",
    "            else:\n",
    "                data=xr.open_mfdataset(config.url)\n",
    "            time_vals=data.time.values\n",
    "            first_timestamp = time_vals[0].astype('datetime64[s]').astype(int)\n",
    "            last_timestamp = time_vals[-1].astype('datetime64[s]').astype(int)\n",
    "            first_time = datetime.fromtimestamp(first_timestamp, tz=timezone.utc)\n",
    "            last_time = datetime.fromtimestamp(last_timestamp, tz=timezone.utc)\n",
    "            if first_time <= parsed_date and last_time >= parsed_date:\n",
    "                print(f\"matched{dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f'{e} for {dataset_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60041521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as dateparse\n",
    "from datetime import datetime, timezone\n",
    "from data.utils import get_data_vars_from_equation, time_index_to_datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd4b5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "314f52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB path: /data/db/canso500.sqlite3\n",
      "Sample variable: sos\n",
      "Tables: [('Dimensions',), ('Variables',), ('VarsDims',), ('Filepaths',), ('Timestamps',), ('TimestampVariableFilepath',)]\n",
      "Total rows in Timestamps: 48\n",
      "Rows for variable 'sos': 48\n",
      "First few timestamps: [(2380928400,), (2380932000,), (2380935600,), (2380939200,), (2380942800,)]\n",
      "get_timestamps size: 48\n",
      "First/last from get_timestamps: 2380928400 2381097600\n"
     ]
    }
   ],
   "source": [
    "from dateutil.parser import parse as dateparse\n",
    "import numpy as np\n",
    "\n",
    "dataset_id_list = DatasetConfig.get_datasets()\n",
    "target_date = '2025-07-23T02:30:00.000Z'\n",
    "parsed_date = dateparse(target_date)\n",
    "\n",
    "\n",
    "\n",
    "# pick a variable robustly\n",
    "sample_var = getattr(config, \"sample_variable\", None) or (config.variables[0] if config.variables else None)\n",
    "print(\"DB path:\", config.url)\n",
    "print(\"Sample variable:\", sample_var)\n",
    "\n",
    "if not sample_var:\n",
    "    raise RuntimeError(\"No variables found in this dataset config\")\n",
    "\n",
    "with SQLiteDatabase(config.url) as db:\n",
    "    # Sanity: list tables\n",
    "    tables = db.c.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
    "    print(\"Tables:\", tables)\n",
    "\n",
    "    # Do we even have any timestamps at all?\n",
    "    ts_count = db.c.execute(\"SELECT COUNT(*) FROM Timestamps\").fetchone()[0]\n",
    "    print(\"Total rows in Timestamps:\", ts_count)\n",
    "\n",
    "    # How many rows for THIS variable?\n",
    "    cnt_for_var = db.c.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM TimestampVariableFilepath tvf\n",
    "        JOIN Variables v ON tvf.variable_id = v.id\n",
    "        JOIN Timestamps t ON tvf.timestamp_id = t.id\n",
    "        WHERE v.variable = ?\n",
    "    \"\"\", (sample_var,)).fetchone()[0]\n",
    "    print(f\"Rows for variable '{sample_var}':\", cnt_for_var)\n",
    "\n",
    "    # Peek a few timestamps for this var (ordered)\n",
    "    first_last = db.c.execute(\"\"\"\n",
    "        SELECT t.timestamp\n",
    "        FROM TimestampVariableFilepath tvf\n",
    "        JOIN Variables v ON tvf.variable_id = v.id\n",
    "        JOIN Timestamps t ON tvf.timestamp_id = t.id\n",
    "        WHERE v.variable = ?\n",
    "        ORDER BY t.timestamp ASC\n",
    "        LIMIT 5\n",
    "    \"\"\", (sample_var,)).fetchall()\n",
    "    print(\"First few timestamps:\", first_last)\n",
    "\n",
    "    # What your code was doing:\n",
    "    vals = np.array(db.get_timestamps(sample_var))\n",
    "    print(\"get_timestamps size:\", vals.size)\n",
    "    if vals.size == 0:\n",
    "        print(\"No timestamps returned by get_timestamps(sample_var) -> this caused your index error.\")\n",
    "    else:\n",
    "        print(\"First/last from get_timestamps:\", vals[0], vals[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "517dabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DatasetConfig(dataset_id_list[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df4092e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matchedgiops_fc_10d_2dll\n",
      "[real_datetime(2024, 7, 22, 3, 0, tzinfo=<UTC>), real_datetime(2025, 8, 2, 0, 0, tzinfo=<UTC>)] vozocrtx\n",
      "matchedciops-east_fc_3dll\n",
      "[real_datetime(2025, 7, 23, 0, 0, tzinfo=<UTC>), real_datetime(2025, 7, 23, 3, 0, tzinfo=<UTC>)] vozocrtx\n",
      "matchedciops-east_fc_2dll\n",
      "[real_datetime(2025, 7, 23, 0, 0, tzinfo=<UTC>), real_datetime(2025, 7, 23, 3, 0, tzinfo=<UTC>)] vozocrtx\n",
      "matchedciops-west_fc_3dll\n",
      "[real_datetime(2025, 7, 23, 1, 0, tzinfo=<UTC>), real_datetime(2025, 7, 23, 3, 0, tzinfo=<UTC>)] vozocrtx\n",
      "matchedciops-west_fc_2dll\n",
      "[real_datetime(2025, 7, 23, 1, 0, tzinfo=<UTC>), real_datetime(2025, 7, 23, 3, 0, tzinfo=<UTC>)] vozocrtx\n",
      "matchedciops-salish-sea_fc_3dll\n",
      "[real_datetime(2025, 7, 23, 1, 0, tzinfo=<UTC>), real_datetime(2025, 7, 24, 20, 0, tzinfo=<UTC>)] vozocrtx\n",
      "matchedciops-salish-sea_fc_2dll\n",
      "[real_datetime(2025, 7, 23, 1, 0, tzinfo=<UTC>), real_datetime(2025, 7, 24, 20, 0, tzinfo=<UTC>)] vozocrtx\n",
      "matchedsalishseacast_3d_biology\n",
      "matchedsalishseacast_3d_currents\n"
     ]
    }
   ],
   "source": [
    "dataset_id_list=DatasetConfig.get_datasets()\n",
    "target_date='2025-07-23T02:30:00.000Z'\n",
    "parsed_date = dateparse(target_date)\n",
    "\n",
    "for dataset_id in dataset_id_list:\n",
    "    try:\n",
    "        config = DatasetConfig(dataset_id)\n",
    "        if not isinstance(config.url,list) and config.url.endswith(\".sqlite3\"):\n",
    "            with SQLiteDatabase(config.url) as db:\n",
    "                for var in config.variables:\n",
    "                    vals = np.array(db.get_timestamps(var))\n",
    "                    if vals.size > 0:\n",
    "                        break\n",
    "                time_dim_units = config.time_dim_units\n",
    "                converted_times = time_index_to_datetime(\n",
    "                    vals[[0, -1]], time_dim_units\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                    converted_times[0] <= parsed_date\n",
    "                    and converted_times[1] >= parsed_date\n",
    "                ):\n",
    "                    print(f\"matched{dataset_id}\")\n",
    "                    print(converted_times, var)\n",
    "        else:\n",
    "            if not isinstance(config.url, list):\n",
    "                data=xr.open_mfdataset([config.url])\n",
    "            else:\n",
    "                data=xr.open_mfdataset(config.url)\n",
    "            time_vals=data.time.values\n",
    "            first_timestamp = time_vals[0].astype('datetime64[s]').astype(int)\n",
    "            last_timestamp = time_vals[-1].astype('datetime64[s]').astype(int)\n",
    "            first_time = datetime.fromtimestamp(first_timestamp, tz=timezone.utc)\n",
    "            last_time = datetime.fromtimestamp(last_timestamp, tz=timezone.utc)\n",
    "            if first_time <= parsed_date and last_time >= parsed_date:\n",
    "                print(f\"matched{dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f'{e} for {dataset_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afcc71df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[real_datetime(2024, 6, 1, 0, 0, tzinfo=<UTC>), real_datetime(2025, 6, 12, 17, 0, tzinfo=<UTC>)]\n",
      "riops_fc_3dps\n"
     ]
    }
   ],
   "source": [
    "dataset_id_list=DatasetConfig.get_datasets()\n",
    "target_date='2025-07-23T02:30:00.000Z'\n",
    "parsed_date = dateparse(target_date)\n",
    "config = DatasetConfig(dataset_id_list[3])\n",
    "sample_var=config.variables[3]\n",
    "with SQLiteDatabase(config.url) as db:\n",
    "    vals = np.array(db.get_timestamps(sample_var))\n",
    "time_dim_units = config.time_dim_units\n",
    "converted_times = time_index_to_datetime(\n",
    "    vals[[0, -1]], time_dim_units\n",
    "                )\n",
    "print(converted_times)\n",
    "print(dataset_id_list[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b1ba19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vozocrte',\n",
       " 'vomecrtn',\n",
       " 'magwatervel',\n",
       " 'votemper',\n",
       " 'vosaline',\n",
       " 'sspeed',\n",
       " 'density',\n",
       " 'deepsoundchannel',\n",
       " 'soniclayerdepth',\n",
       " 'deepsoundchannelbottom',\n",
       " 'psubsurfacechannel']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d69dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navigator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
